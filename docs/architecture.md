# üèóÔ∏è Architecture Overview

## Table of Contents
- [High-Level Architecture](#high-level-architecture)
- [Medallion Architecture Pattern](#medallion-architecture-pattern)
- [Component Details](#component-details)
- [Data Flow](#data-flow)
- [Security & Governance](#security--governance)
- [Technology Stack](#technology-stack)

---

## High-Level Architecture
```mermaid
graph TB
    subgraph "Data Sources"
        A[Kaggle Dataset<br/>100k Orders]
    end
    
    subgraph "Ingestion Layer"
        B[Python Upload Script<br/>boto3]
    end
    
    subgraph "Storage Layer - AWS S3"
        C1[Bronze Layer<br/>Raw Data]
        C2[Silver Layer<br/>Cleaned Data]
        C3[Gold Layer<br/>Business Metrics]
    end
    
    subgraph "Processing - Databricks"
        D1[Bronze Processing<br/>PySpark Ingestion]
        D2[Silver Processing<br/>PySpark Transformations]
        D3[Gold Processing<br/>PySpark Aggregations]
    end
    
    subgraph "Quality & Governance"
        E1[Great Expectations<br/>Data Quality]
        E2[AWS IAM<br/>Access Control]
        E3[CloudWatch<br/>Monitoring]
    end
    
    subgraph "Orchestration"
        F[Apache Airflow<br/>Workflow Automation]
    end
    
    subgraph "Analytics"
        G[Databricks SQL<br/>Dashboard]
    end
    
    A -->|CSV Files| B
    B -->|Upload| C1
    C1 -->|Read| D1
    D1 -->|Validate| E1
    D1 -->|Write| C2
    C2 -->|Read| D2
    D2 -->|Quality Check| E1
    D2 -->|Write| C3
    C3 -->|Read| D3
    D3 -->|Write| C3
    C3 -->|Query| G
    
    F -.->|Trigger| D1
    F -.->|Trigger| D2
    F -.->|Trigger| D3
    
    E2 -.->|Control| C1
    E2 -.->|Control| C2
    E2 -.->|Control| C3
    
    E3 -.->|Monitor| D1
    E3 -.->|Monitor| D2
    E3 -.->|Monitor| D3
    
    style A fill:#e1f5ff
    style C1 fill:#fff4e6
    style C2 fill:#e8f5e9
    style C3 fill:#fce4ec
    style D1 fill:#fff9c4
    style D2 fill:#fff9c4
    style D3 fill:#fff9c4
    style E1 fill:#f3e5f5
    style F fill:#e0f2f1
    style G fill:#e8eaf6
```

---

## Medallion Architecture Pattern

### **Why Medallion Architecture?**

The **Medallion Architecture** (aka **Multi-Hop Architecture**) is a modern data design pattern for data platforms used to organize data logically.
The goal is to incermentally and progressively improve the structure and quality of data as it flows through each layer of the architecture (from Bronze ‚áí Silver ‚áí Gold layer tables).

### **The Three Layers:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BRONZE LAYER (Raw Zone)                                    ‚îÇ
‚îÇ  Purpose: Ingestion & Historical Archive                    ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  ‚Ä¢ Raw data, immutable                                      ‚îÇ
‚îÇ  ‚Ä¢ Format: Parquet                                          ‚îÇ
‚îÇ  ‚Ä¢ Partition: date, source                                  ‚îÇ
‚îÇ  ‚Ä¢ Immutable (append-only)                                  ‚îÇ
‚îÇ  ‚Ä¢ Schema validation                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SILVER LAYER (Refined Zone)                                ‚îÇ
‚îÇ  Purpose: Cleaned, Conformed, Enriched                      ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  ‚Ä¢ Deduplication                                            ‚îÇ
‚îÇ  ‚Ä¢ Type casting & validation                                ‚îÇ
‚îÇ  ‚Ä¢ Tables  joins                                            ‚îÇ
‚îÇ  ‚Ä¢ Enrich data (geo, temporal features)                     ‚îÇ
‚îÇ  ‚Ä¢ Data quality checks (Great Expectations)                 ‚îÇ
‚îÇ  ‚Ä¢ Slowly Changing Dimensions (SCD Type 2)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GOLD LAYER (Curated Zone)                                  ‚îÇ
‚îÇ  Purpose: Business-Level Aggregations                       ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  ‚Ä¢ KPIs e-commerce (CLV, churn, etc.)                       ‚îÇ
‚îÇ  ‚Ä¢ Pre-aggregated data for dashboards                       ‚îÇ
‚îÇ  ‚Ä¢ Star schema / Data Mart                                  ‚îÇ
‚îÇ  ‚Ä¢ Query optimised for BI                                   ‚îÇ
‚îÇ  ‚Ä¢ Business documentation                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Benefits:**

| Benefit | Description |
|---------|-------------|
| **Incremental Processing** | Process only new data (delta), not full reloads |
| **Data Quality Gates** | Validate at each layer before promotion |
| **Time Travel** | Keep historical versions for audit/recovery |
| **Performance** | Pre-aggregated gold tables = fast dashboards |
| **Flexibility** | Re-process silver/gold without touching bronze |
| **Collaboration** | Clear boundaries between data engineers & analysts |

---

## Component Details

### **1. Data Sources**

#### **Kaggle E-Commerce Dataset (Olist)**
- **Source:** Brazilian E-Commerce Public Dataset
- **Period:** 2016-2018
- **Records:** ~100,000 orders
- **Tables:** 8 relational tables

**Schema:**
```
orders (99,441 rows)
‚îú‚îÄ‚îÄ order_id (PK)
‚îú‚îÄ‚îÄ customer_id (FK)
‚îú‚îÄ‚îÄ order_status
‚îú‚îÄ‚îÄ order_purchase_timestamp
‚îî‚îÄ‚îÄ ...

order_items (112,650 rows)
‚îú‚îÄ‚îÄ order_id (FK)
‚îú‚îÄ‚îÄ product_id (FK)
‚îú‚îÄ‚îÄ price
‚îî‚îÄ‚îÄ freight_value

customers (99,441 rows)
products (32,951 rows)
sellers (3,095 rows)
product_category (71 rows)
geolocation (1,000,163 rows)
order_reviews (99,224 rows)
order_payments (103,886 rows)
```

### **2. Ingestion Layer**

#### **Python Upload Script**
```python
# scripts/upload_to_s3.py
- Reads CSV files from data/raw/
- Uploads to S3 bronze/ with partitioning
- Handles incremental uploads (delta detection)
- Error handling & retry logic
- Logging to CloudWatch
```

**Features:**
- ‚úÖ Batch upload with multipart for large files
- ‚úÖ Data validation before upload
- ‚úÖ Idempotent (can re-run safely)
- ‚úÖ Progress tracking

### **3. Storage Layer - AWS S3**

#### **Bucket Structure:**
```
s3://kaggle-e-commerce-datalake-us-east-1/
‚îÇ
‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date=2024-02-19/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ orders.parquet
‚îÇ   ‚îú‚îÄ‚îÄ customers/
‚îÇ   ‚îú‚îÄ‚îÄ products/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îú‚îÄ‚îÄ orders_cleaned/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date=2024-02-19/
‚îÇ   ‚îú‚îÄ‚îÄ customer_enriched/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îú‚îÄ‚îÄ customer_lifetime_value/
‚îÇ   ‚îú‚îÄ‚îÄ daily_revenue/
‚îÇ   ‚îú‚îÄ‚îÄ product_performance/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ archive/
    ‚îî‚îÄ‚îÄ (data > 90 days via lifecycle policy)
```

**S3 Features Used:**
- **Partitioning:** Improved query performance
- **Lifecycle Policies:** Auto-archive to Glacier after 90 days
- **Versioning:** Recovery from accidental deletes
- **Encryption:** SSE-S3 (Server-Side Encryption)

### **4. Processing - Databricks (PySpark)**

#### **Bronze Processing**
```python
# notebooks/bronze/01_ingest_orders.py

Purpose: Ingest raw CSV to Parquet with minimal transformation

Steps:
1. Read CSV from local/S3
2. Infer schema (or apply explicit schema)
3. Add metadata columns:
   - ingestion_timestamp
   - source_file
   - data_quality_flag
4. Write to bronze/ as Parquet (partitioned by date)
5. Log metrics (row count, file size, duration)
```

#### **Silver Processing**
```python
# notebooks/silver/02_transform_orders.py

Purpose: Clean, validate, and enrich data

Transformations:
1. Deduplication (based on order_id + timestamp)
2. Type casting (strings ‚Üí dates, decimals)
3. Null handling (drop/impute based on business rules)
4. Outlier detection (price < 0, shipping_days > 365)
5. Join with dimension tables (customers, products)
6. Add computed columns:
   - delivery_time_days
   - is_delayed (boolean)
   - customer_segment (RFM)
7. Data quality validation (Great Expectations)
8. Write to silver/ with SCD Type 2 for historical tracking
```

#### **Gold Processing**
```python
# notebooks/gold/03_aggregate_metrics.py

Purpose: Create business-ready KPIs

Aggregations:
1. Customer Lifetime Value (CLV)
   - Total revenue per customer
   - Average order value
   - Purchase frequency

2. Product Performance
   - Revenue by category
   - Top sellers
   - Inventory turnover

3. Delivery SLA
   - On-time delivery rate
   - Average delivery time by region
   - Carrier performance

4. Churn Analysis
   - Days since last purchase
   - Predicted churn probability (features for ML)

5. Daily/Monthly Revenue
   - Time series aggregations
   - Year-over-year growth
```

### **5. Data Quality & Governance**

#### **Great Expectations**

**Expectation Suites:**
```yaml
# Bronze Layer Expectations
orders_bronze_suite:
  - expect_column_to_exist: order_id
  - expect_column_values_to_be_unique: order_id
  - expect_column_values_to_not_be_null: order_id
  - expect_column_values_to_be_of_type: 
      column: order_purchase_timestamp
      type_: datetime

# Silver Layer Expectations
orders_silver_suite:
  - expect_column_values_to_be_between:
      column: price
      min_value: 0
      max_value: 10000
  - expect_column_values_to_be_in_set:
      column: order_status
      value_set: [delivered, shipped, canceled, processing]
  - expect_table_row_count_to_be_between:
      min_value: 90000
      max_value: 110000
```

#### **AWS IAM (Role-Based Access Control)**
```hcl
# infrastructure/aws/iam.tf

Roles:
- data-engineer-role (Read/Write bronze, silver, gold)
- data-analyst-role (Read-only gold)
- airflow-role (Execute jobs, write logs)

Policies:
- S3 least-privilege access (specific buckets only)
- CloudWatch write-only logs
- Databricks service account (programmatic access)
```

### **6. Orchestration - Apache Airflow**

**DAG Structure:**
```python
# pipelines/dags/daily_ecommerce_pipeline.py

DAG: daily_ecommerce_etl
Schedule: 0 2 * * * (Daily at 2 AM UTC)

Tasks:
[Start] 
  ‚Üí [Validate S3 bronze files exist]
  ‚Üí [Run bronze_ingestion (Databricks job)]
  ‚Üí [Data quality checks (Great Expectations)]
  ‚Üí [Run silver_transformations (Databricks job)]
  ‚Üí [Data quality checks]
  ‚Üí [Run gold_aggregations (Databricks job)]
  ‚Üí [Update dashboard metadata]
  ‚Üí [Send success notification (email/Slack)]
[End]

Error Handling:
- Retry 3 times with exponential backoff
- Alert on failure via Slack webhook
- Quarantine bad data to separate S3 path
```

### **7. Analytics - Databricks SQL**

**Dashboard KPIs:**

1. **Executive Summary**
   - Total Revenue (MTD, YTD)
   - Active Customers
   - Average Order Value
   - Delivery Performance %

2. **Product Analytics**
   - Top 10 products by revenue
   - Category performance
   - Inventory levels

3. **Customer Insights**
   - Customer segmentation (RFM)
   - Cohort analysis
   - Churn rate

4. **Operational Metrics**
   - Pipeline SLA (processing time)
   - Data freshness (last update timestamp)
   - Error rate

---

## Data Flow

### **End-to-End Flow Example**
```
1. USER ACTION (2016-09-04 21:15:19)
   ‚îî‚îÄ> Customer places order on e-commerce platform

2. TRANSACTIONAL SYSTEM
   ‚îî‚îÄ> Order data written to OLTP database

3. DATA EXPORT (simulated by Kaggle dataset)
   ‚îî‚îÄ> CSV export: orders.csv, order_items.csv, etc.

4. INGESTION (Daily @ 2 AM)
   ‚îî‚îÄ> Python script uploads CSV ‚Üí S3 bronze/
   ‚îî‚îÄ> Format: s3://kaggle-e-commerce-datalake-us-east-1/bronze/orders/date=2024-02-19/orders.parquet

5. BRONZE PROCESSING (2:05 AM)
   ‚îî‚îÄ> Databricks reads S3 bronze
   ‚îî‚îÄ> Minimal validation (schema check)
   ‚îî‚îÄ> Writes back to bronze/ as Parquet (immutable)

6. SILVER PROCESSING (2:15 AM)
   ‚îî‚îÄ> Reads bronze Parquet
   ‚îî‚îÄ> Cleans: dedup, type cast, null handling
   ‚îî‚îÄ> Enriches: joins customers, products
   ‚îî‚îÄ> Validates: Great Expectations suite
   ‚îî‚îÄ> Writes to silver/ (overwrite daily partition)

7. GOLD PROCESSING (2:30 AM)
   ‚îî‚îÄ> Reads silver tables
   ‚îî‚îÄ> Aggregates: daily revenue, CLV, churn features
   ‚îî‚îÄ> Writes to gold/ (append to time series)

8. ANALYTICS (Real-time)
   ‚îî‚îÄ> Databricks SQL Dashboard queries gold/
   ‚îî‚îÄ> Business users see updated KPIs
   ‚îî‚îÄ> Analysts run ad-hoc SQL queries

9. GOVERNANCE
   ‚îî‚îÄ> CloudWatch logs all operations
   ‚îî‚îÄ> Data lineage tracked in metadata
   ‚îî‚îÄ> Audit trail for compliance
```

---

## Security & Governance

### **Security Layers**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Network Security                                   ‚îÇ
‚îÇ  ‚Ä¢ VPC with private subnets (if production)         ‚îÇ
‚îÇ  ‚Ä¢ Security groups (allow only necessary ports)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Identity & Access (IAM)                            ‚îÇ
‚îÇ  ‚Ä¢ Least-privilege roles                            ‚îÇ
‚îÇ  ‚Ä¢ MFA enforced for admin access                    ‚îÇ
‚îÇ  ‚Ä¢ Service accounts with temporary credentials      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Security                                      ‚îÇ
‚îÇ  ‚Ä¢ S3 encryption at rest (SSE-S3)                   ‚îÇ
‚îÇ  ‚Ä¢ Encryption in transit (TLS 1.2+)                 ‚îÇ
‚îÇ  ‚Ä¢ No PII in bronze/silver (anonymized in gold)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Secrets Management                                 ‚îÇ
‚îÇ  ‚Ä¢ Never commit credentials to Git                  ‚îÇ
‚îÇ  ‚Ä¢ .env files (local) + .gitignore                  ‚îÇ
‚îÇ  ‚Ä¢ AWS Secrets Manager (production)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Monitoring & Auditing                              ‚îÇ
‚îÇ  ‚Ä¢ CloudWatch logs (all S3 access)                  ‚îÇ
‚îÇ  ‚Ä¢ CloudTrail (API calls audit)                     ‚îÇ
‚îÇ  ‚Ä¢ Data lineage (source ‚Üí destination tracking)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Data Governance Practices**

| Practice | Implementation |
|----------|----------------|
| **Data Catalog** | Databricks Unity Catalog (metadata + lineage) |
| **Data Quality** | Great Expectations (automated validation) |
| **Schema Evolution** | Parquet schema merging (backward compatible) |
| **Retention Policy** | S3 Lifecycle: archive after 90 days ‚Üí Glacier |
| **Disaster Recovery** | S3 versioning + cross-region replication (if critical) |
| **Compliance** | GDPR-ready (PII anonymization, right to deletion) |

---

## Technology Stack

### **Complete Stack Overview**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CLOUD INFRASTRUCTURE                                   ‚îÇ
‚îÇ  ‚Ä¢ AWS S3 (Data Lake)                                   ‚îÇ
‚îÇ  ‚Ä¢ AWS IAM (Access Control)                             ‚îÇ
‚îÇ  ‚Ä¢ AWS CloudWatch (Monitoring)                          ‚îÇ
‚îÇ  ‚Ä¢ AWS Glue Catalog (Metadata - optional)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  COMPUTE & PROCESSING                                   ‚îÇ
‚îÇ  ‚Ä¢ Databricks Community (PySpark runtime)               ‚îÇ
‚îÇ  ‚Ä¢ Python 3.9+ (scripting, automation)                  ‚îÇ
‚îÇ  ‚Ä¢ Apache Spark 3.5.0 (distributed processing)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DATA TRANSFORMATION                                    ‚îÇ
‚îÇ  ‚Ä¢ dbt-core (SQL-based transformations)                 ‚îÇ
‚îÇ  ‚Ä¢ PySpark DataFrames (complex logic)                   ‚îÇ
‚îÇ  ‚Ä¢ Delta Lake format (ACID transactions - optional)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DATA QUALITY & TESTING                                 ‚îÇ
‚îÇ  ‚Ä¢ Great Expectations (validation framework)            ‚îÇ
‚îÇ  ‚Ä¢ pytest (unit tests)                                  ‚îÇ
‚îÇ  ‚Ä¢ data-diff (comparing datasets)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ORCHESTRATION                                          ‚îÇ
‚îÇ  ‚Ä¢ Apache Airflow 2.8+ (workflow automation)            ‚îÇ
‚îÇ  ‚Ä¢ GitHub Actions (CI/CD)                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INFRASTRUCTURE AS CODE                                 ‚îÇ
‚îÇ  ‚Ä¢ Terraform (AWS provisioning)                         ‚îÇ
‚îÇ  ‚Ä¢ boto3 (Python SDK for AWS)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  VERSION CONTROL & COLLABORATION                        ‚îÇ
‚îÇ  ‚Ä¢ Git (version control)                                ‚îÇ
‚îÇ  ‚Ä¢ GitHub (repository hosting)                          ‚îÇ
‚îÇ  ‚Ä¢ Conventional Commits (standardized messages)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ANALYTICS & VISUALIZATION                              ‚îÇ
‚îÇ  ‚Ä¢ Databricks SQL (interactive queries)                 ‚îÇ
‚îÇ  ‚Ä¢ Streamlit (optional dashboards)                      ‚îÇ
‚îÇ  ‚Ä¢ Power BI / Tableau (via S3 export)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Development Environment**
```yaml
IDE: VSCode
Extensions:
  - Python
  - Databricks
  - Terraform
  - GitLens
  - Rainbow CSV
  - SQLTools

Python Version: 3.9+
Virtual Environment: venv
Package Manager: pip
Linter: pylint, flake8
Formatter: black
```

---

## Design Principles

### **1. Idempotency**
> Running the same pipeline multiple times produces the same result

- Bronze: Append-only (never overwrite)
- Silver/Gold: Partition overwrite (daily partitions)
- All operations have unique IDs (order_id, ingestion_timestamp)

### **2. Incremental Processing**
> Process only new data, not full table scans

- Bronze: Date partitioning (process only new dates)
- Silver: Delta detection (compare timestamps)
- Gold: Incremental aggregations (merge, not rebuild)

### **3. Data Quality First**
> Validate early, fail fast

- Schema validation at bronze ingestion
- Business rule checks at silver transformation
- Anomaly detection before gold promotion
- Quarantine bad data (don't drop, investigate)

### **4. Documentation as Code**
> Documentation lives with the code

- ADRs for major decisions
- Inline comments for complex logic
- dbt docs auto-generated from SQL
- Architecture diagrams versioned in Git

### **5. Observability**
> Know what's happening in production

- Structured logging (JSON format)
- Metrics: row counts, processing time, error rate
- Alerts: data freshness SLA, pipeline failures
- Data lineage: source ‚Üí destination tracking

---

## Future Enhancements

### **Phase 2 (Next 3 months)**
- [ ] Implement Delta Lake for ACID transactions
- [ ] Add ML features for churn prediction
- [ ] Real-time streaming with Kafka
- [ ] dbt incremental models

### **Phase 3 (6 months)**
- [ ] Multi-region replication
- [ ] Data mesh architecture (domain-oriented)
- [ ] Advanced anomaly detection
- [ ] Self-service analytics portal

---

## References

- [Medallion Architecture - Databricks](https://www.databricks.com/glossary/medallion-architecture)
- [Data Quality with Great Expectations](https://greatexpectations.io/)
- [AWS S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)

---

**Last Updated:** 2024-02-22  
**Author:** [Hatem BEN SALEM]  
**Version:** 1.0